**Update on tasks:**

At this point, we have completed all the main preparation tasks outlined in our project plan, and our progress is on schedule.
We first created the full repository structure described in our plan. The folders /data/raw, /scripts were all set up as planned. We uploaded our two raw datasets into /data/raw/ and removed placeholder files like .gitkeep, and cleaned the structure so that everything is easy to navigate. This gives us a clear starting point for later steps such as cleaning, integration, and analysis.
Next, we completed the data acquisition work. For the calorie supply dataset from Our World in Data , we created a Python script called acquire_data.py in the /scripts folder. This script automatically downloads the Our World in Data CSV file using the requests library, creates any necessary folders, saves the dataset into /data/raw, and computes its SHA-256 hash to support data integrity. The obesity dataset cannot be downloaded programmatically because it is from Kaggle which requires us to log in in order to download it, so we uploaded it manually and documented this limitation. Even though it was uploaded manually, we still computed its SHA-256 hash to ensure reproducibility and to follow the project requirements.
We also wrote a second script, verify_checksums.py, which verifies the integrity of all raw datasets using the hashes stored in a checksums.txt file located at the root of the repository. This script reads each fileâ€™s expected hash, compares it to the actual hash on disk, and reports whether the file matches or is missing. This step satisfies the requirement of having scripts that help others reproduce our exact data and verify that nothing has changed. We are currently working on writing clear documentation that explains how someone else can acquire our datasets including instructions for downloading the Our World in Data file, manually obtaining the Kaggle file and verifying file integrity using the provided SHA-256 checksums. This documentation will be added to the /docs folder in the next milestone. 

**Team contributions:**

Tracie Huynh thuyn3: For team contributions during this milestone, I completed most of the technical setup and scripting tasks. I created the repository structure, uploaded the raw datasets, wrote the data acquisition script acquire_data.py, implemented SHA-256 integrity functions, created the checksum file checksums.txt, wrote the verification script verify_checksums.py, cleaned up folder issues, and documented the data collection and acquisition steps. I also helped drafting this milestone report and made sure that we are on track. Also, I tested all of my scripts inside Visual Studio Code to make sure they run correctly and produce the expected outputs. I also checked the raw datasets inside VS Code to confirm their structure and make sure they are ready for cleaning and integration in the next milestone. I also spent time fixing path issues and making sure our directory layout matches the project requirements. 

Cynthia Shen xs49: My focus at this stage has been establishing the reproducibility and data integrity framework, and initiating the Data Profiling stage. I co-led the initial GitHub repository setup and directory structure creation. My current technical work involves assessing the raw datasets (daily-per-capita-caloric-supply.csv and obesity-cleaned.csv) for integration readiness. I have confirmed two critical standardization needs in the obesity data: 1) Filtering to isolate the 'Both sexes' data as required by the research question, and 2) Extracting the mean obesity percentage from the column that includes confidence intervals (e.g., 20.4 [15.9-25.7]). I have begun compiling the necessary Country Name mapping to ensure a successful join. I am now formalizing these findings into the required docs/data_profile.md and developing the cleaning script.
